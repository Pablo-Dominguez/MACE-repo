---
title: "Seeds"
author: "Pablo Dominguez"
date: '`r Sys.Date()`'
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    collapsed: true
    css: style.css
    code_folding: hide
    includes:
      in_header: header.html
editor_options: 
  chunk_output_type: console
---

```{r message = FALSE}
library(readxl)
library(dplyr)
library(knitr)
library(ggplot2)
library(ggpubr)
library(pastecs)
library(kableExtra)
library (e1071)
library(cvms)
```


La información completa sobre la generación de este documento puede encontrarse en este [repositorio de GitHub](https://github.com/Pablo-Dominguez/MACE-repo).

# Entrega MACE bloque 2

## Introducción

Por su riqueza en proteínas, carbohidratos, lípidos y minerales las semillas de calabaza son un producto que llega a ser consumido a lo largo de países en todo el mundo. Los factores que determinan el tipo concreto de cada semilla son generalmente relativos a la apariencia externa de la misma. Distinguir el tipo de cada semilla suele requerir experiencia y tiempo, cosa que en nuestro mundo moderno se traduce como un coste económico. 

Con el objetivo de aportar una nueva manera de realizar esta tarea, nos proponemos plantear un algoritmo de clasificación binaria tipo `SVM` que, a partir de la base de datos `Pumpkin_Seeds_Dataset.xlsx`, distinga entre dos categorías diferentes de semillas de calabaza.

Así pues, comenzaremos este estudio con un análisis exploratorio de los datos, para seguir con una construcción de un clasificador a partir de este algoritmo y acabar con una evaluación de los diferentes resultados obtenidos.

## Base de datos

### Presentación de los datos

Tal y como se muestra a continuación, tenemos las dos categorías balanceadas de semillas de calabaza *Çerçevelik* y *Ürgüp Sivrisi*: 

```{r class.source = 'fold-show'}
options(scipen=999) # desactivamos la notación científica
options(max.print=100) # limitamos la longitud de los print

df <- readxl::read_excel("../dbs/Pumpkin_Seeds_Dataset/Pumpkin_Seeds_Dataset.xlsx")
table(df$Class)
```

> Citation Request :
KOKLU, M., SARIGIL, S., & OZBEK, O. (2021). The use of machine learning methods in classification of pumpkin seeds (Cucurbita pepo L.). Genetic Resources and Crop Evolution, 68(7), 2713-2726. Doi: https://doi.org/10.1007/s10722-021-01226-0

> Puedes conseguir la base de datos en [este enlace](https://www.muratkoklu.com/datasets/Pumpkin_Seeds_Dataset.zip).



### Variables a considerar

En el caso de nuestro estudio, salvo que alguna de las variables tenga poca calidad de dato e introduzca ruido, conservaremos todas para que el modelo disponga de la mayor cantidad de información. El significado de las variables de las que disponemos es por lo general evidente a partir del nombre de la misma. Mostramos a continuación la lista de variables que se han medido para cada muestra, así como el tipo de cada variable en nuestra base de datos:

```{r}
df %>% sapply(class) %>% kable(.,col.names = c("Tipo"))
```

<br>
Para continuar con el estudio, analizaremos los valores faltantes y los outliers o valores atípicos, así como presentaremos histogramas comparativos de cada variable  para cada tipo de semilla.

### Calidad del dato

En cuanto a los valores faltantes, tenemos la siguiente información:

```{r}
for(col in df %>% colnames()){
  print(paste0("Hay ",df[col] %>% is.na() %>% sum()," valores faltantes y ",df[col] %>% is.null() %>% sum()," valores nulos en la columna ",col))
}
```

Con lo cual, afortunadamente no tenemos que preocuparnos por técnicas de sustitución o eliminación de valores faltantes o nulos, gracias a que la base de datos está completa.
Comprobemos ahora los valores atípicos. Para ello, pasaremos a presentar un diagrama de cajas y bigotes para cada variable junto con diagramas tipo violin, así como el porcentaje de este tipo de valores junto con los principales estadísticos. De este modo, tendremos una mejor visión de la distribución y el comportamiento de las variables para cada clase de semilla.

```{r out.width="100%"}
ps <- list()
for(colu in df %>% colnames() %>% setdiff(.,"Class")){
  p <- ggplot(df, aes_string(x="Class", y=colu, color="Class")) + geom_violin() + geom_boxplot(width=0.25) + stat_boxplot(geom = "errorbar", width = 0.2) + theme(
    axis.text.x = element_blank(),
    axis.text.y = element_text(size=6),
    axis.title.x = element_text(size = 8),
    axis.title.y = element_text(size = 8),
    legend.key.size = unit(0.1, 'cm'),
    legend.text = element_text(size=8),
    legend.title = element_blank())
  ps[[colu]] <- p
}
ggarrange(plotlist =  ps, nrow = 4,ncol = 3,common.legend = TRUE)
```

Así mismo, existen tres aproximaciones estándar a la hora de tratar este tipo de valores:

1. **Eliminar los outliers**. Esta técnica se recomienda cuando el porcentaje de outliers es muy alto, usualmente en torno al 10%. El motivo de emplear esta técnica es que si un dato tiene muchos outliers, es probable que sean debidos a ruido o mala calidad del dato.
2. **Capado de los outliers**. Se limita el valor máximo y mínimo posible que pueden tomar los valores a dos percentiles dados, por ejemplo al percentil 10 y al 90. Esta técnica es preferible a la anterior por resultar menos agresiva a la hora de impartar sobre el resto de estadísticos.
3. **Sustitución de outliers**. Finalmente, existe la técnica de sustituir los outliers por la mediana. 

A continuación, calculamos los estadísticos principales para cada categoría.

```{r}

# Definimos un df para cada categoría

cat.vec <- df$Class %>% table() %>% names()
df_class01 <- df %>% filter(.,Class == cat.vec[1]) # Çerçevelik
df_class02 <- df %>% filter(.,Class == cat.vec[2]) # Ürgüp Sivrisi

# Calculamos los cuantiles para cada clase

## Clase 1

col_numeric <- which( sapply(df_class01, is.numeric ) ) # Creamos un selector de las columnas numéricas
quantile_df01 <- sapply(col_numeric, function( y ) {
                      quantile( x = unlist( df_class01[,  y ] ), c(.1,.25,.5,.75,.9))
                      }) %>% as.data.frame()
rownames(quantile_df01) <- c("P10","Q1","Q2","Q3","P90")

## Clase 2

col_numeric <- which( sapply(df_class02, is.numeric ) ) # Creamos un selector de las columnas numéricas
quantile_df02 <- sapply(col_numeric, function( y ) {
                      quantile( x = unlist( df_class02[,  y ] ), c(.1,.25,.5,.75,.9))
                      }) %>% as.data.frame()
rownames(quantile_df02) <- c("P10","Q1","Q2","Q3","P90")

# Calculamos el % de outliers para cada clase

## Clase 1

Q1 <- quantile_df01['Q1',] %>% as.vector() %>% as.numeric()
Q3 <- quantile_df01['Q3',] %>% as.vector() %>% as.numeric()
top_IQR <- Q3+1.5*(Q3-Q1) #111027.750000
bot_IQR <- Q1-1.5*(Q3-Q1)
k <- 1
perc_out01 <- c()
for(col in head(colnames(df_class01),-1)){
  count_top_out <- df_class01[col]>=top_IQR[k] %>% as.vector()
  count_bot_out <- df_class01[col]<=bot_IQR[k] %>% as.vector()
  perc_out01 <- c(perc_out01,100*(sum(count_top_out)+sum(count_bot_out))/(df_class01 %>% nrow()))
  k <- k+1
} # el vector perc_out representa el porcentaje de outliers de cada variable.

## Clase 2

Q1 <- quantile_df02['Q1',] %>% as.vector() %>% as.numeric()
Q3 <- quantile_df02['Q3',] %>% as.vector() %>% as.numeric()
top_IQR <- Q3+1.5*(Q3-Q1) 
bot_IQR <- Q1-1.5*(Q3-Q1)
k <- 1
perc_out02 <- c()
for(col in head(colnames(df_class02),-1)){
  count_top_out <- df_class02[col]>=top_IQR[k] %>% as.vector()
  count_bot_out <- df_class02[col]<=bot_IQR[k] %>% as.vector()
  perc_out02 <- c(perc_out02,100*(sum(count_top_out)+sum(count_bot_out))/(df_class02 %>% nrow()))
  k <- k+1
} # el vector perc_out representa el porcentaje de outliers de cada variable.


# Creamos el df final con todos los estadísticos para cada clase

## Calculamos los estadísticos para la clase 1

desc_df01 <- stat.desc(df_class01[,-which(names(df_class01) == "Class")]) %>% slice(., c(4,5,8,9,10,13))  

## Añadimos los porcentajes de outliers para el dataframe de la clase 1

perc_out_class01 <- data.frame(t(perc_out01))
colnames(perc_out_class01) <-  colnames(desc_df01)
rownames(perc_out_class01) <- c("perc.out") #

fn1='Más información sobre los principales estadísticos en `?pastecs::stat.desc`.'
fn2='`perc.out`: Es el porcentaje de outliers para cada variable, según el [criterio de Tukey](https://es.wikipedia.org/wiki/Diagrama_de_caja).'

desc_class01 <- desc_df01 %>%  bind_rows(.,perc_out_class01)
desc_class01  %>% kbl() %>% kable_minimal("hover", full_width = F)%>%
  footnote(number=c(fn1,fn2), number_title="Estadísticos Clase Çerçevelik", title_format = "bold") %>%  scroll_box(width = "auto")

## Calculamos los estadísticos para la clase 2

desc_df02 <- stat.desc(df_class02[,-which(names(df_class02) == "Class")]) %>% slice(., c(4,5,8,9,10,13))  

## Añadimos los porcentajes de outliers para el dataframe de la clase 2

perc_out_class02 <- data.frame(t(perc_out02))
colnames(perc_out_class02) <-  colnames(desc_df02)
rownames(perc_out_class02) <- c("perc.out") #

desc_class02 <- desc_df02 %>%  bind_rows(.,perc_out_class02)
desc_class02  %>% kbl() %>% kable_minimal("hover", full_width = F)%>%
  footnote(c(""),general_title="Estadísticos Clase Ürgüp Sivrisi", title_format = "bold") %>%  scroll_box(width = "auto")
```

<br>
Dado que en ambas categorías la variable que mayor porcentaje de outliers presenta es la `Solidity` con un 4% de outliers,vamos a proceder a sustituirlos por el percentil 10 y el 90, respectivamente y a volver a calcular los estadísticos para comparar si hemos modificado en gran medida la estructura de las variables.


```{r}

# Reemplazamos los outliers para cada clase

## Clase 1

Q1 <- quantile_df01['Q1',] %>% as.vector() %>% as.numeric()
Q3 <- quantile_df01['Q3',] %>% as.vector() %>% as.numeric()
P10 <- quantile_df01['P10',] %>% as.vector() %>% as.numeric()
P90 <- quantile_df01['P90',] %>% as.vector() %>% as.numeric()
top_IQR <- Q3+1.5*(Q3-Q1) #111027.750000
bot_IQR <- Q1-1.5*(Q3-Q1)

trimmed_df_class01 <- data.frame(df_class01)
perc_out01 <- c()
perc_mod01 <- c()
for(i in 1:(ncol(trimmed_df_class01)-1)) { 
  top_mod <- trimmed_df_class01[ , i][trimmed_df_class01[ , i] <= bot_IQR[i]]%>% length()
  bot_mod <- trimmed_df_class01[ , i][trimmed_df_class01[ , i] >= top_IQR[i]]%>% length()
  
  trimmed_df_class01[ , i][trimmed_df_class01[ , i] <= bot_IQR[i]] <- P10[i]
  trimmed_df_class01[ , i][trimmed_df_class01[ , i] >= top_IQR[i]] <- P90[i]
  
  count_top_out <- trimmed_df_class01[ , i]>=top_IQR[i] %>% as.vector()
  count_bot_out <- trimmed_df_class01[ , i]<=bot_IQR[i] %>% as.vector()
  
  perc_out01 <- c(perc_out01,100*(sum(count_top_out)+sum(count_bot_out))/(trimmed_df_class01 %>% nrow()))
  perc_mod01 <- c(perc_mod01,100*(top_mod+bot_mod)/(trimmed_df_class01 %>% nrow()))
  
}# el vector perc_out representa el porcentaje de outliers de cada variable.

## Clase 2

Q1 <- quantile_df02['Q1',] %>% as.vector() %>% as.numeric()
Q3 <- quantile_df02['Q3',] %>% as.vector() %>% as.numeric()
P10 <- quantile_df02['P10',] %>% as.vector() %>% as.numeric()
P90 <- quantile_df02['P90',] %>% as.vector() %>% as.numeric()
top_IQR <- Q3+1.5*(Q3-Q1) #111027.750000
bot_IQR <- Q1-1.5*(Q3-Q1)

trimmed_df_class02 <- data.frame(df_class02)
perc_out02 <- c()
perc_mod02 <- c()
for(i in 1:(ncol(trimmed_df_class02)-1)) { 
  top_mod <- trimmed_df_class02[ , i][trimmed_df_class02[ , i] <= bot_IQR[i]]%>% length()
  bot_mod <- trimmed_df_class02[ , i][trimmed_df_class02[ , i] >= top_IQR[i]]%>% length()
  
  trimmed_df_class02[ , i][trimmed_df_class02[ , i] <= bot_IQR[i]] <- P10[i]
  trimmed_df_class02[ , i][trimmed_df_class02[ , i] >= top_IQR[i]] <- P90[i]
  
  count_top_out <- trimmed_df_class02[ , i]>=top_IQR[i] %>% as.vector()
  count_bot_out <- trimmed_df_class02[ , i]<=bot_IQR[i] %>% as.vector()
  
  perc_out02 <- c(perc_out02,100*(sum(count_top_out)+sum(count_bot_out))/(trimmed_df_class02 %>% nrow()))
  perc_mod02 <- c(perc_mod02,100*(top_mod+bot_mod)/(trimmed_df_class02 %>% nrow()))
  
}# el vector perc_out representa el porcentaje de outliers de cada variable.

# Creamos el df final con todos los estadísticos para cada clase

## Calculamos los estadísticos para la clase 1

desc_df01 <- stat.desc(trimmed_df_class01[,-which(names(trimmed_df_class01) == "Class")]) %>% slice(., c(4,5,8,9,10,13))  

## Añadimos los porcentajes de outliers y el porcentaje de valores modificados para el dataframe de la clase 1

perc_out_class01 <- data.frame(t(perc_out01))
colnames(perc_out_class01) <-  colnames(desc_df01)
rownames(perc_out_class01) <- c("perc.out") #

perc_mod_class01 <- data.frame(t(perc_mod01))
colnames(perc_mod_class01) <-  colnames(desc_df01)
rownames(perc_mod_class01) <- c("perc.mod") #

fn2='`perc.mod`: Es el porcentaje de valores modificados para cada variable.'

desc_class01 <- desc_df01 %>%  bind_rows(.,perc_out_class01) %>% bind_rows(.,perc_mod_class01)
desc_class01  %>% kbl() %>% kable_minimal("hover", full_width = F)%>%
  footnote(number=c(fn2), number_title ="Estadísticos Clase Çerçevelik - Modificado", title_format = "bold") %>%  scroll_box(width = "auto")

## Calculamos los estadísticos para la clase 2

desc_df02 <- stat.desc(trimmed_df_class02[,-which(names(trimmed_df_class02) == "Class")]) %>% slice(., c(4,5,8,9,10,13))  

## Añadimos los porcentajes de outliers para el dataframe de la clase 2

perc_out_class02 <- data.frame(t(perc_out02))
colnames(perc_out_class02) <-  colnames(desc_df02)
rownames(perc_out_class02) <- c("perc.out") #

perc_mod_class02 <- data.frame(t(perc_mod02))
colnames(perc_mod_class02) <-  colnames(desc_df02)
rownames(perc_mod_class02) <- c("perc.mod") #

desc_class02 <- desc_df02 %>%  bind_rows(.,perc_out_class02)%>% bind_rows(.,perc_mod_class02)
desc_class02  %>% kbl() %>% kable_minimal("hover", full_width = F)%>%
  footnote(c(""),general_title="Estadísticos Clase Ürgüp Sivrisi - Modificado", title_format = "bold") %>%  scroll_box(width = "auto")

```

<br>
Como se puede observar respectivamente en cada tabla, hemos eliminado en todos los casos los outliers al tiempo que no hemos afectado significativamente el resto de estadísticos. Emplearemos por tanto esta base de datos modificada como base de datos de partida para el proceso del modelado de los datos.

```{r class.source = 'fold-show'}
# Creación de la base de datos para el modelado.
df_final <- trimmed_df_class01 %>% bind_rows(.,trimmed_df_class02)
```


## Modelización con `SVM`

### Modelización inicial

En primer lugar, debemos configurar la variable `Class` para que nuestro modelo la considere como un factor. Además, es en este momento donde vamos a crear la división entre nuestro conjunto de entrenamiento y nuestro conjunto de test.

```{r class.source = 'fold-show'}

saved_dbs <- list.files("../dbs/model_db/")

if("df_final.Rds" %in% saved_dbs){
  df_final <- readRDS(file="../dbs/model_db/df_final.Rds")
} else {
  df_final$Class <- as.factor(df_final$Class)
  saveRDS(df_final, file="../dbs/model_db/df_final.Rds")
}

# Creamos los conjuntos de entrenamiento y test
df_final$id <- 1:nrow(df_final)
set.seed(210)
train <- df_final %>% dplyr::sample_frac(.75)
test  <- dplyr::anti_join(df_final, train, by = 'id')

# Eliminamos la columna id
train <- train[,!(names(train) %in% c("id"))]
test <- test[,!(names(test) %in% c("id"))]

```

Comprobamos a continuación que las clases de semilla estén balanceadas dentro de cada conjunto.

```{r}
table(train$Class) %>% print()
table(test$Class) %>% print()
```



Una vez hechas las preparaciones oportunas, vamos a crear una serie de modelos con los que iniciaremos nuestra fase de modelado. Crearemos cuatro tuneados, uno para cada tipo de kernel, con un vector inicial de costes con los valores `0.001, 0.01, 0.1, 1, 5, 10, 100` y 10 pliegues de validación cruzada. De entre ellos, seleccionaremos los dos tipos kernels que arrojen mejores resultados y haremos un tuneo más en profundidad sobre los hiperparámetros de cada tipo de modelo respectivamente.

Cabe destacar en esta parte del proceso dos puntos sobre los modelos que presentamos a continuación. Por una parte, en general la búsqueda de hiperparámetros es un proceso costoso, sobre todo en términos de tiempo de computación, por lo que el siguiente bloque de código se limita a cargar los modelos que están almacenados en [la carpeta](https://github.com/Pablo-Dominguez/MACE-repo/tree/main/models/base) `models/base` previamente ejecutados y guardados. De esta forma, podemos generar el presente documento sin necesidad de reentrenar los modelos cada vez.
Por otro lado, debido además a que nuestro ámbito de estudio se centra en presentar las capacidades de `SVM` no realizaremos una búsqueda exhaustiva de los hiperparámetros óptimos, si no una muestra de algunas de las estrategias para encontrarlos.

```{r}

# Listado de modelos iniciales guardados

saved_models <- list.files("../models/base/")

# Modelado inicial con kernel lineal

if("tuned_linmod01.Rds" %in% saved_models){
  tuned_linmod01 <- readRDS("../models/base/tuned_linmod01.Rds")
  print("Modelado inicial con kernel lineal")
  tuned_linmod01_sum <- summary(tuned_linmod01)
  tuned_linmod01_sum %>% print()
  print(paste0("Error promedio: ",tuned_linmod01_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_linmod01_sum$performances$error %>% sd()))
} else {
  tuned_linmod01 <- tune(svm ,Class ~ ., data = train, kernel = "linear",
                       ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ),
                       tunecontrol = tune.control(cross=10))
  saveRDS(tuned_linmod01,"../models/base/tuned_linmod01.Rds")
  tuned_linmod01 <- readRDS("../models/base/tuned_linmod01.Rds")
  print("Modelado inicial con kernel lineal")
  tuned_linmod01_sum <- summary(tuned_linmod01)
  tuned_linmod01_sum %>% print()
  print(paste0("Error promedio: ",tuned_linmod01_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_linmod01_sum$performances$error %>% sd()))
}

# Modelado inicial con kernel polinomial

if("tuned_polymod01.Rds" %in% saved_models){
  tuned_polymod01 <- readRDS("../models/base/tuned_polymod01.Rds")
  print("Modelado inicial con kernel polinomial")
  tuned_polymod01_sum <- summary(tuned_polymod01)
  print(tuned_polymod01_sum)
  print(paste0("Error promedio: ",tuned_polymod01_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_polymod01_sum$performances$error %>% sd()))
} else {
  tuned_polymod01 <- tune(svm ,Class ~ ., data = train, kernel = "polynomial",
                       ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ),
                       degree = 3,
                       tunecontrol = tune.control(cross=10))
  saveRDS(tuned_polymod01,"../models/base/tuned_polymod01.Rds")
  tuned_polymod01 <- readRDS("../models/base/tuned_polymod01.Rds")
  print("Modelado inicial con kernel polinomial")
  tuned_polymod01_sum <- summary(tuned_polymod01)
  print(tuned_polymod01_sum)
  print(paste0("Error promedio: ",tuned_polymod01_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_polymod01_sum$performances$error %>% sd()))
}

# Modelado inicial con kernel radial

if("tuned_radmod01.Rds" %in% saved_models){
  tuned_radmod01 <- readRDS("../models/base/tuned_radmod01.Rds")
  print("Modelado inicial con kernel radial")
  tuned_radmod01_sum <- summary(tuned_radmod01)
  print(tuned_radmod01_sum)
  print(paste0("Error promedio: ",tuned_radmod01_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_radmod01_sum$performances$error %>% sd()))
} else {
  tuned_radmod01 <- tune(svm ,Class ~ ., data = train, kernel = "radial",
                       ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ),
                       tunecontrol = tune.control(cross=10))
  saveRDS(tuned_radmod01,"../models/base/tuned_radmod01.Rds")
  tuned_radmod01 <- readRDS("../models/base/tuned_radmod01.Rds")
  print("Modelado inicial con kernel radial")
  tuned_radmod01_sum <- summary(tuned_radmod01)
  print(tuned_radmod01_sum)
  print(paste0("Error promedio: ",tuned_radmod01_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_radmod01_sum$performances$error %>% sd()))
}

# Modelado inicial con kernel sigmoidal

if("tuned_sigmod01.Rds" %in% saved_models){
  tuned_sigmod01 <- readRDS("../models/base/tuned_sigmod01.Rds")
  print("Modelado inicial con kernel sigmoidal")
  tuned_sigmod01_sum <- summary(tuned_sigmod01)
  print(tuned_sigmod01_sum)
  print(paste0("Error promedio: ",tuned_sigmod01_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_sigmod01_sum$performances$error %>% sd()))
} else {
  tuned_sigmod01 <- tune(svm ,Class ~ ., data = train, kernel = "sigmoid",
                       ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ),
                       tunecontrol = tune.control(cross=10))
  saveRDS(tuned_sigmod01,"../models/base/tuned_sigmod01.Rds")
  tuned_sigmod01 <- readRDS("../models/base/tuned_sigmod01.Rds")
  print("Modelado inicial con kernel sigmoidal")
  tuned_sigmod01_sum <- summary(tuned_sigmod01)
  print(tuned_sigmod01_sum)
  print(paste0("Error promedio: ",tuned_sigmod01_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_sigmod01_sum$performances$error %>% sd()))
}

```

Así, de estas cuatro primeras aproximaciones, vamos a realizar un tuneo con algo más de profundidad para los modelos con **kernel lineal** y **con kernel radial**. La decisión de centrarnos en estos dos tipos, en el caso del kernel lineal viene justificada por ser el que menor error promedio tiene y se comporta más flexiblemente (tiene mayor sesgo y menor varianza) frente a los cambios de coste. 

En el caso del radial se justifica porque, aunque presenta un error similar con respecto al kernel polinomial, este está influenciado por el error para el coste `0.001`. No obstante esto, es posible que si realizásemos una búsqueda de hiperparámetros con mayor exhaustividad exista un modelo con kernel polinomial más óptimo que el radial seleccionado. Nos limitamos a esta elección, además de apoyada por los argumentos presentados, con el objetivo de limitar el alcance de este estudio.

### Tuneo de hiperparámetros

Así pues, realizamos a continuación una búsqueda en mayor profundidad de los hiperparámetros para los kernels `lineal` y `radial`. Centraremos la búsqueda en aquellos valores de los costes que nos hayan aportado mejor resultado.

En el caso del kernel `lineal`, vemos que obtenemos errores bajos para los valores del coste entre 1 y 10, por lo que nos centraremos en costes en este entorno. 

```{r}
# Listado de modelos tuneados guardados

saved_models <- list.files("../models/tuned/")

# Modelado con kernel lineal

if("tuned_linmod02.Rds" %in% saved_models){
  tuned_linmod02 <- readRDS("../models/tuned/tuned_linmod02.Rds")
  print("Modelado con kernel lineal")
  tuned_linmod02_sum <- summary(tuned_linmod02)
  tuned_linmod02_sum %>% print()
  print(paste0("Error promedio: ",tuned_linmod02_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_linmod02_sum$performances$error %>% sd()))
} else {
  tuned_linmod02 <- tune(svm ,Class ~ ., data = train, kernel = "linear",
                       ranges =list(cost=seq(from = 1, to = 10, by=0.25) ),
                       tunecontrol = tune.control(cross=10))
  saveRDS(tuned_linmod02,"../models/tuned/tuned_linmod02.Rds")
  tuned_linmod01 <- readRDS("../models/tuned/tuned_linmod02.Rds")
  print("Modelado con kernel lineal")
  tuned_linmod02_sum <- summary(tuned_linmod02)
  tuned_linmod02_sum %>% print()
  print(paste0("Error promedio: ",tuned_linmod02_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_linmod02_sum$performances$error %>% sd()))
}
```

En general modelos más sencillos como es el caso del `SVM lineal` que aquí presentamos tendrán mayor sesgo y menor varianza. De este modo, podemos comprobar que el error en este caso se comporta como era esperable. Así, elegiremos arbitrariamente como primer candidato para nuestro modelado al mejor modelo con kernel lineal obtenido, el modelo `tuned_linmod02$best.model` con coste `3.5`.

Por otra parte, en el caso del kernel `radial`, vamos a aplicar una validación cruzada para determinar el mejor valor para los hiperparámetros del coste y de la `gamma`.

```{r}

# Modelado con kernel radial

if("tuned_radmod02.Rds" %in% saved_models){
  tuned_radmod02 <- readRDS("../models/tuned/tuned_radmod02.Rds")
  print("Modelado con kernel radial")
  tuned_radmod02_sum <- summary(tuned_radmod02)
  print(tuned_radmod02_sum)
  print(paste0("Error promedio: ",tuned_radmod02_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_radmod02_sum$performances$error %>% sd()))
} else {
  tuned_radmod02 <- tune(svm ,Class ~ ., data = train, kernel = "radial",
                       ranges =list(cost=seq(from = 5, to = 10, by=0.5),
                                    gamma=seq(from = 0.1, to = 5, by=0.2) ),
                       tunecontrol = tune.control(cross=10))
  saveRDS(tuned_radmod02,"../models/tuned/tuned_radmod02.Rds")
  tuned_radmod02 <- readRDS("../models/tuned/tuned_radmod02.Rds")
  print("Modelado con kernel radial")
  tuned_radmod02_sum <- summary(tuned_radmod02)
  print(tuned_radmod02_sum)
  print(paste0("Error promedio: ",tuned_radmod02_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_radmod02_sum$performances$error %>% sd()))
}
```

Al igual que en el caso anterior, vamos a seleccionar el mejor modelo `tuned_radmod02$best.model` que nos devuelve la validación cruzada, con coste `6.5` y gamma `0.1`. Destacar que aunque ha conllevado un tiempo de ejecución considerable, la búsqueda del gamma no ha arrojado mejores resultados que los iniciales.   

### Propuesta de mejora

A pesar de que las decisiones tomadas anteriormente sobre las elecciones de estrategias, parámetros y modelos son lícitas, hemos visto que existe un margen de mejora en cuanto a la rigurosidad con la que hemos realizado la búsqueda de hiperparámetros. Es por esto que, con la intención de obtener resultados más refinados, nos basaremos en propuestas que podemos encontrar en la bibliografía para comparar con los modelos iniciales obtenidos.

Es por esto que, según se aconseja en [[A Practical Guide to Support Vector Classification](#Ref01)], vamos a realizar una normalización de las variables junto con una búsqueda de hiperparámetros sobre una secuencia de tipo exponencial. Compararemos el modelo que obtengamos de este proceso contra los dos obtenidos anteriormente.

Así, comenzamos por estandarizar los conjuntos de entrenamiento y test haciendo uso de la misma media y varianza, esto es, las del entrenamiento. El motivo de hacer uso de estos estadísticos en ambos casos es que de otro modo estaríamos "filtrando" información al modelo a la hora de aplicarlo sobre el conjunto test.

```{r class.source = 'fold-show'}

# Estandarizamos el conjunto de entrenamiento.
st_train <- scale(train[,1:(ncol(train)-1)],center = TRUE, scale = TRUE) %>% as.data.frame()
st_train$Class <- train$Class # Espero estar incluyendo la clase en el mismo orden. Revisar.

# Calculamos el vector de medias y el vector de desviaciones estándar del conjunto train.

means <- train %>% summarise_if(is.numeric, mean)
st.devs <- train %>% summarise_if(is.numeric, sd)

# Restamos las medias y dividimos por las desviaciones estándar
st_test <- scale(test[,1:(ncol(test)-1)],center = means, scale = st.devs) %>% as.data.frame()
st_test$Class <- test$Class 

```

Realizamos a continuación un secuenciado exponencial sobre los hiperparámetros. Como nota, hemos reducido la validación cruzada a cinco pliegues para que el tiempo de computación no sea excesivo.

```{r}
# Modelado refinado con kernel radial

if("tuned_radmod03.Rds" %in% saved_models){
  tuned_radmod03 <- readRDS("../models/tuned/tuned_radmod03.Rds")
  print("Modelado refinado con kernel radial")
  tuned_radmod03_sum <- summary(tuned_radmod03)
  print(tuned_radmod03_sum)
  print(paste0("Error promedio: ",tuned_radmod03_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_radmod03_sum$performances$error %>% sd()))
} else {
  tuned_radmod03 <- tune(svm ,Class ~ ., data = st_train, kernel = "radial",
                       ranges =list(cost=2**seq(from=-5, to=10),
                                    gamma=2**seq(from=-5, to=10)),
                       tunecontrol = tune.control(cross=5))
  saveRDS(tuned_radmod03,"../models/tuned/tuned_radmod03.Rds")
  tuned_radmod03 <- readRDS("../models/tuned/tuned_radmod03.Rds")
  print("Modelado refinado con kernel radial")
  tuned_radmod03_sum <- summary(tuned_radmod03)
  print(tuned_radmod03_sum)
  print(paste0("Error promedio: ",tuned_radmod03_sum$performances$error %>% mean(),
               ". Desv. típica del error: ", tuned_radmod03_sum$performances$error %>% sd()))
}
```

Del mismo modo que en los casos anteriores, seleccionaremos el mejor modelo `tuned_radmod03$best.model` que nos devuelve la validación cruzada, con coste `64` y gamma `0.03125`. Con esto tenemos nuestro tercer y último candidato de modelo `SVM` con el que evaluar los resultados contra el conjunto test.

<!-- 
svmfit <-  svm(st_train$Class~., data=st_train, kernel ="radial",gamma =0.03125, cost=64)
plot(tuned_radmod03$best.model , st_train, Perimeter ~ Area)
plot(tuned_radmod03$best.model , st_train, Perimeter ~ Convex_Area)
plot(svmfit , st_train,  Area ~ Perimeter) # Class must be a factor

probar a ejecutar el svmfit que no salga del tuneado, lo construimos de nuevo.
 -->

```{r}
plot(tuned_radmod03$best.model , st_train, Perimeter ~ Convex_Area)
plot(tuned_radmod02$best.model , st_train, Perimeter ~ Convex_Area)
```


## Comparación de los resultados

En esta sección final del proceso de modelado de los datos, vamos a presentar una serie de dimensiones o perspectivas del error cometido por cada modelo. Para ello, haremos uso de la **matriz de confusión** de cada uno de los modelos sobre el conjunto de entrenamiento.

```{r}
confusion_table = confusion_matrix(targets = st_test[,"Class"], 
                                   predictions = predict(tuned_radmod03$best.model,st_test))
plot_confusion_matrix(confusion_table$`Confusion Matrix`[[1]], palette = "Greens")
```


## Conclusiones

## Referencias

### Bibliografía

1. [A Practical Guide to Support Vector Classification ](https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf){#Ref01}
2. [Semi-Supervised Classification by Low Density Separation - Section 2.3.2 ](http://www.gatsby.ucl.ac.uk/aistats/fullpapers/198.pdf){#Ref02}

### Recursos y enlaces de interés

* [The Data Science process - Analytixlabs](https://www.analytixlabs.co.in/blog/data-science-process)
* [Practical guide to outlier detection - Towards data science](https://towardsdatascience.com/practical-guide-to-outlier-detection-methods-6b9f947a161e)
* [Normalization and standardization of data - Analytics Vidhya ](https://www.analyticsvidhya.com/blog/2020/04/feature-scaling-machine-learning-normalization-standardization/)
* [Normalization vs Standardization — Quantitative analysis - Towards data science](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)
* [RStudio cheatsheets](https://www.rstudio.com/resources/cheatsheets/)


